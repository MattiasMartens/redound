- Backpressure in the case of multiple subscribers
  - Principle of least astonishment: probably manage backpressure independently for each subscriber. In some cases this could cause sync issues where two subscribers feed into a single consumer; that's where the runtime clock comes in.
- Errors: we want a clean, standardized paradigm for handling event processing errors and when to terminate sources based on failures in their downstream. It's already established that errors don't propagate directly, but that it is a separate concern handled by the owner of all application sources and sinks.
- Push: nice to be able to synthesize events at a source and then push them upstream to a source so as to induce the data change that would have produced the event. Probably not a concern except in the context of source-capable derivations.
- Source capability: some derivations may use IO in the implementation of their underlying data structure. What this means, in effect, is that their derived data is cached and persists beyond the current runtime. This is useful in the context of extremely large datasets, where sources must use a pull-focused approach instead of querying everything, and some derivations exist that must aggregate source data that is not available in the current runtime. In this case, it also becomes valuable to apply queries to the derivation directly, instead of applying them to the source and waiting for their effects to propagate down. (Of course only the latter approach guarantees that the values are up-to-date, unless the framework user takes extra precautions.) But this also raises the question of if pull-capable derivations should also be push-capable. If they are push-capable, then the derived data can be updated directly as the user wishes to change it. But this means that a derivation can get ahead of its source, or even diverge from its source, which could lead to horrible chaos. This is basically the problem of a computed value with a setter, but in a distributed context. The way to avoid all this is for derivation push operations to always push to their sources, not themselves. The derivation is then updated in the normal way as the source propagates. But between the necessary source and the derivation lies any number of intermediate derivations. So finding the true sources is non-trivial. Also, derivations are meant to be optimally generic and composable. Requiring them to be aware of their sources introduces harmful coupling. For all these reasons, push capability on derivations seems like a bad idea. If push capability is desired, the derivation's downstream part should graduate to a source, and the derivation's upstream part should instead be a sink that generates push events on the new source.
6. The Redound paradigm should produce unidirectional graphs, hence, no cycles. However, it is hard in general to identify this so the framework can complain preemptively and accurately instead of breaking.
7. It's inevitable that apps will write to the same data source they read from. In most cases this is fine. However, it raises the possibility of apps creating infinite loops whose connections extend beyond the current runtime. Hopefully Redound will make this less likely, not more likely. But this should be considered.
8. 'seal' capability: sources should be able to unilaterally indicate that they will not yield any more events. The controller can then decide whether to end the source with an Outcome.
9. Broad-scope tasks: when a query ends, or a source is sealed, the controller may need to wait until all sinks have fully processed the events before capturing an outcome. A derivation is obligated to emit events in order of receipt, but it's not obligated to re-emit all events. How does the sink know that it's received everything it's going to? The only robust solution that comes to mind is for the 'seal' flag to also propagate downstream. But this too poses problems in the case of a derivation that accepts multiple sources. Should a derivation pass on 'seal' if just one source is sealed? Proposal: a so-called 'non-event' type is needed to express the fact that an event is deliberately not passed downstream so that the downstream clocks can be updated. From the perspective of implementors, this is a bloody pain. The abstract contract can require this but there should be a wrapper that encapsulates the guarantee that, once a Derivation's 'consume' promise is resolved, all events have therefore been processed. Then the framework can emit the non-events as necessary. (The main case where this is relevant is Batch transform, where the Derivation must emit events outside of the normal backpressure loop. Also - because of the existence of things like the Batch transform, the non-event paradigm does not totally obviate the propagation of Seal.)
10. To infer when events are fully received, it is necessary for events to carry a 'provenance' that maps their sources to the highest event tick the event is derived from. This works because Derivations must emit events in order of receipt, so the highest tick is the only one that's needed (including 'void' events). It may, however, introduce considerable overhead.
11. Do we care about allowing links in the graph to be asynchronous? In theory, the Redound paradigm should support distributed systems. But the clock stamp concept is not strong enough to support this: asynchronous events may be transmitted through a medium that causes them to arrive out-of-order, falsifying guarantees that are necessary to the integrity of the whole. Solution: each source must have its own unique clock, rather than a global clock. Then the identity of events is derived not from the tick alone but from the tick plus the identity of its source. In this case, if there is an asynchronous bridge that manages a subscription, that bridge can use contiguity to enforce ordering (by keeping sequences in a queue until they become contiguous).
12. Need to wind the faculty for a system-wide ender/error-recoverer into all instances. In general, when something emits an event, that can't fail--the consumer may transmit an error to the sytem-wide, which may result in the source being closed, but the emit execution itself will resolve.
13. Loose end about whether the system-wide controller need truly be system-wide, i.e., is there only one per runtime or might there be multiple sharing different resources?
14. Basically all functions in the core implementation need to delegate *all* errors to their controller, if present. Not implemented yet.
15. Event contracts. 'Consumes' and 'Emits' are currently required, but nothing is done with them. The idea is to warn about compatibility issues between graph components ahead of time. What should a consumer do when it receives an event not registered in Consumes -- ignore it, or throw an error? Should there also be a 'Rejects' list? Or a map of event specs to policies (ACCEPT, IGNORE, REJECT) plus a default policy (ACCEPT, IGNORE, REJECT)? Probably the last one.
16. Concept of 'siphoning'. Usually, it is desirable that a Source should only start fetching data to emit events when it is connected to a Sink. But Derivations blur this distinction because an exterior concern may rely directly on the derived value. In this case, it needs to manually create that pressure by calling a function (perhaps just to create a no-op Sink?). At present, any subscriber creates siphon pressure, even Derivations.
17. Possibility of creating a 'listener', that is, a Sink which does not create siphon pressure, does not close when its Source closes, and does not communicate with the Source's Controller. This could be implemented as an encapsulated Sink which conveys events to an encapsulated Source outside of normal Redound flow.
18. In some cases, perhaps desirable to 'squash' all of the provenance stuff. This can be done to a single Source by emitting a provenance tick of Infinity for all events. It might also be desirable to cut out the overhead of tracking provenances, perhaps with a VoidProvenance singleton that graph components can choose to emit in place. Of course, any controller that is trying to listen for query results or graph lifecycle outcomes would need to detect the VoidProvenance singleton and recognize that its mission is impossible. (There might[???] also be rare cases where the system does not prohibit Provenance but a Derivation needs to emit an event with no Provenance. This is the opposite of a void event, which has a Provenance but no payload. The easiest way to handle this it seems would be for a Derivation to emit a Provenance which is an empty Map. But if this is implemented, pains must be taken to ensure that a Redound developer can't mix up the two.)
19. Attempting run a simple example, found big problems with asynchrony.
- Upstream emit() calls have to wait for backpressure. How does this work when the implementor of generate() does not necessarily have to do so? A backpressure implementation has been made, but not tested or brought into the structure yet.
- seal() events have to wait for backpressure from emit to clear before being sent downstream. This, at least, should be easy once the above works: the seal method can add itself to the same backpressure queue. Further events will not be added because that is the definition of seal().
- close() does not wait for its turn: it shuts everything off immediately. This means that the implementations of Source and Derivation must not emit events after they are closed. Every emitter must therefore clear its backpressure queue on close. The implementations of Derivation's consume() and seal() methods, and Source's generate() method, should avoid emitting events after they are closed.
- The framework is open-ended about when downstream events may be yielded up: the implementor receives an emit() function. It is explicitly not required that emit() only be called during the span of the implementor call's initial Promise. But this creates problems. First, how does a Derivation avoid emitting events out of order when it does not know the outcome of previous consume() calls? As a solution, consider changing the above. Instead of receiving an emit() function, Derivations return a synchronous generator that may or may not yield a Promise. If the generator yields a Promise, the event is then only emitted downstream when that Promise successfully resolves (if it fails, the event is voided, and the failure is passed to the Controller). This Promise does not apply backpressure to Sources, but it does apply backpressure to the Derivation itself: events received while waiting for the Promise to resolve will not emit until that Promise has resolved. Similarly, of course, the Seal event will not be propagated downstream while the Derivation is waiting on such a Promise. The Derivation must therefore maintain a queue of not-yet-exhausted Generators. (As an intermediate step, try making it work with Derivations completely synchronous.)
- To determine when a Query has finished, Sinks listen for a Query's highest clock tick propagating fully through the network. Given that a derivation may emit multiple events based on a given Source clock tick, how does the Sink know when the final one has been processed? Perhaps the best way is for an event with clock ticks to also have a "HighestOfTick" flag, indicating that it is the final consequence of the Source events it tracks. For a Source this is always true. A Derivation that receives an event with "HighestOfTick" being false must also emit false based on it, but if "HighestOfTick" is true, it must emit true for only the last event it will emit based on it.
- An author of Derivations should not have to worry too much about the implementations of provenance and "HighestOfTick". Some of these complexities are difficult to conceal however, at least in the case where async generators come into play.
- Possibility of unifying Source queries with Derivation queries. i.e., a Derivation can 'respond' to a query by emitting events to that consumer. If it can't respond, it propagates it up the chain. Queries should not be divisible: if a graph component has a query that could be answered in parts, it should be split into smaller Queries. Queries can be implemented as Dictionaries.
- Both Queries-as-Dictionaries, if implemented, and Sources-as-Dictionaries (already implemented) should support Symbols in some way. This would help to keep future libraries encapsulated.
20. Thinking of splitting the current Derivation up into subtypes.
- "Transform". Applies backpressure always.
- "Relay". Applies backpressure only if a write is in progress. Always upon receiving an event, generates a Promise representing when the event will be fully processed and writes that Promise to a queue. (These Promises are tracked for the purpose of tagging. They resolve to whatever is to be emitted downstream. Handling the cases of batch() will require special attention here. It will need to make use of deferred Promises stored in the Derivation's Aggregate.) To manage the complexity for programmers, expectation is that Relays will not alter the data they receive, only pass it through after some kind of delay OR batch it into a data structure such as a list. (Examples of Relays: Debouncer {after a delay, squash all events before the most recent and emit the most recent}, Throttler {emit an event, but squash all events that arrive within a certain time window of that event}, Batcher {squash no events, but compile them into lists and emit the lists after a delay or when they get too big}, Limiter {squash no events, but keep them in a queue and emit them at fixed intervals}).
- "Balancer". Expectation is that all its subscribers are homomorphic: they receive the same events and carry out the same logic on those events. The Balancer applies backpressure at the level of individual subscribers. If a subscriber is available, it receives the event immediately without waiting for other subscribers to clear. Balancer is probably best conceived as a standard component, with minimal configuration. (Maybe a prioritization function for determining which subscribers to emit to first. Maybe a function for identifying when to apply system backpressure, e.g., maybe start stochastically applying backpressure to events when the load hits a certain percentage of capacity.) (A Balancer could also encapsulate a database connection pool: initialize a Balancer from a pool and all the connections become Sinks. Subscribe/Unsubscribe Sinks from the Balancer as the pool creates or disposes connections.)
- "Multiplexer". At any given time, re-emits events only from some of its Sources and not others. Has an inner State which indicates which sources it listens to, which can dynamically update in response to any event. Uniquely among Derivations, the Multiplexer can add and dispose Sources after derivation, as long as all Sources over its lifecycle have the same Controller. Data transformation is allowed, but no secondary async events.
- "Switchboard???". Emits events only to certain subscribers. Original idea was that subscribers choose based on their own conditions whether to receive an emit (or more properly whether to re-emit it). But there may be cases where the emitter determines this. In these cases it is more parsimonious to enforce it at the Switchboard rather than emitting events with metadata like "Sink-X-Ignore-This-Please". Low priority though. We should wait for best practices to emerge in simpler cases.
(Multiplexer is probably important for web apps. Balancer is probably important for servers.)
- Often have need to copy-paste core functions from one file to another to avoid circular dependencies. This is easily avoided by instance methods so maybe that's the way to go?
- Query Seal Events. The controller wants to be notified when a pull() has fully yielded events and those events have been fully processed by all sinks. In theory, a Sink seal *result* might be useful as well. This raises questions about unnecessary complexity. Aren't Sinks supposed to be the things that communicate with the outside? Why all this controller seal result business? Probably it only makes sense at the end of lifecycle (i.e. seal result), not while the thing is in flight. Then if the sinks are meant to collect data that is useful to other sections of runtime code, the sink should just be implemented as a Subscribable that those sections of code can subscribe to.
- On Derivations, RemainingUnsealedSources should be a Set of roles (as identified by strings), not a Set of the actual SourceInstances.
- Hadn't considered what happens when close() implementations produce errors. Should they supersede the controller's outcome? allSinksClosed() and outcomePromise() should be merged: the Promise should only resolve to the outer when close() is processed throughout, and then it should resolve with either the original Outcome if close() worked, or a Left with an object encapsulating both the components that failed to close and their Errors, and the original Outcome.
- Instead of passing capabilities as arguments to consume(), it might be desirable to have the requests as keys in the returned object. This would help enforce the convention that consume()'s behaviour in a given invocation must not depend on the result of push() and pull() operations resulting from that invocation. Update: actually, capabilities should be passed in, but they should be defined as objects at the source with each function having its own key in a dictionary; they would be easier to type this way.
- Queries. In some cases, in order to conclude a query, a graph component might need to perform a pull() operation. If it waits for the pull() operation to yield events before emitting, it will either have to hold up one of its sources, if the pull() operation calls on a different source; or, even worse, it will deadlock as it waits for itself. The component could hold the necessary data in its Aggregate and only release it when the data arrives, but this later release will not have the Query tag associated with it. It won't hold up the propagation of an EndOfTagEvent and the downstream won't necessarily be able to process it as a result of that query. To solve this, a derivation's return value could include an optional `uponCondition` parameter containing a condition fn `condition()`, an optional boolean `ignoreNonFulfillment`, and an emit function `upon()` with return type `PossiblyAsyncResult<T>`. (Speculative: `timeout` option, `interval` option to check the condition outside of normal event flow.) The following requirements would need to hold:
1. If `uponCondition` is present, the condition is saved internally. For the tag of the event that generated this output, `EndOfTagEvent` is saved instead of being processed.
2. For every other event the component receives, it first fully processes the event, then checks `condition()`. If `condition()` is true, the condition is cleared, `upon()` is called, all results from the call to `upon()` are emitted with the same tag as the original input, and `EndOfTagEvent` for the tag is emitted if it was received previously.
3. If in processing an event the component receives `seal()` with conditions still pending, it checks `ignoreNonFulfillment` for those conditions. If for any of them `ignoreNonFulfillment` is not present or false, it panics and calls the controller with a non-fulfillment error. Otherwise, for all of them, it emits `EndOfTagEvent` before emitting `SealEvent`.
- More options for creating 'tagged' events outside of push() and pull() semantics.